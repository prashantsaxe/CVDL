{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\nprint('hello')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:29.929891Z","iopub.execute_input":"2025-03-31T02:04:29.930220Z","iopub.status.idle":"2025-03-31T02:04:30.433663Z","shell.execute_reply.started":"2025-03-31T02:04:29.930192Z","shell.execute_reply":"2025-03-31T02:04:30.432883Z"}},"outputs":[{"name":"stdout","text":"hello\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"!pip install torch torchvision albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:31.768126Z","iopub.execute_input":"2025-03-31T02:04:31.768415Z","iopub.status.idle":"2025-03-31T02:04:35.214059Z","shell.execute_reply.started":"2025-03-31T02:04:31.768393Z","shell.execute_reply":"2025-03-31T02:04:35.213004Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nRequirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\nRequirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.29.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Load pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify the classifier for custom dataset\nnum_classes = 4  # Example: 3 classes + 1 background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:35.215360Z","iopub.execute_input":"2025-03-31T02:04:35.215673Z","iopub.status.idle":"2025-03-31T02:04:35.907316Z","shell.execute_reply.started":"2025-03-31T02:04:35.215637Z","shell.execute_reply":"2025-03-31T02:04:35.906573Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:35.908745Z","iopub.execute_input":"2025-03-31T02:04:35.909048Z","iopub.status.idle":"2025-03-31T02:04:35.912529Z","shell.execute_reply.started":"2025-03-31T02:04:35.909017Z","shell.execute_reply":"2025-03-31T02:04:35.911671Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"import torchvision.transforms as T\n\nclass CustomDataset(Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms if transforms else T.ToTensor()  # Convert to tensor by default\n        self.imgs = sorted(os.listdir(os.path.join(root, \"images\")))\n        self.annotations = sorted(os.listdir(os.path.join(root, \"annotations\")))\n\n    def load_annotations(self, ann_path):\n        tree = ET.parse(ann_path)\n        root = tree.getroot()\n\n        boxes = []\n        labels = []\n        label_map = {\"with_mask\": 1, \"without_mask\": 2, \"mask_weared_incorrect\": 3}  # Modify as needed\n\n        for obj in root.findall(\"object\"):\n            name = obj.find(\"name\").text\n            labels.append(label_map[name])  \n\n            bndbox = obj.find(\"bndbox\")\n            xmin = int(bndbox.find(\"xmin\").text)\n            ymin = int(bndbox.find(\"ymin\").text)\n            xmax = int(bndbox.find(\"xmax\").text)\n            ymax = int(bndbox.find(\"ymax\").text)\n\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n        annotation_path = os.path.join(self.root, \"annotations\", self.annotations[idx])\n\n        image = Image.open(img_path).convert(\"RGB\")\n        boxes, labels = self.load_annotations(annotation_path)\n\n        target = {\"boxes\": boxes, \"labels\": labels}\n\n        # Convert image to tensor\n        image = self.transforms(image)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.imgs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:35.913401Z","iopub.execute_input":"2025-03-31T02:04:35.913619Z","iopub.status.idle":"2025-03-31T02:04:35.923519Z","shell.execute_reply.started":"2025-03-31T02:04:35.913599Z","shell.execute_reply":"2025-03-31T02:04:35.922830Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((300, 400)),  # Resize all images to 300x400\n    transforms.ToTensor(),  # Convert to tensor\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:38.531462Z","iopub.execute_input":"2025-03-31T02:04:38.531816Z","iopub.status.idle":"2025-03-31T02:04:38.535948Z","shell.execute_reply.started":"2025-03-31T02:04:38.531787Z","shell.execute_reply":"2025-03-31T02:04:38.535172Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Apply the transformation when loading the dataset\ndataset = CustomDataset(\"/kaggle/input/face-mask-detection\", transforms=transform)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:40.889184Z","iopub.execute_input":"2025-03-31T02:04:40.889480Z","iopub.status.idle":"2025-03-31T02:04:40.897232Z","shell.execute_reply.started":"2025-03-31T02:04:40.889456Z","shell.execute_reply":"2025-03-31T02:04:40.896465Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"dataset = CustomDataset(\"/kaggle/input/face-mask-detection\")\nimage, target = dataset[0]\n\nprint(f\"Image Size: {image.size}\")\nprint(f\"Target: {target}\")  # Should print a dictionary with \"boxes\" and \"labels\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:41.586438Z","iopub.execute_input":"2025-03-31T02:04:41.586774Z","iopub.status.idle":"2025-03-31T02:04:41.606806Z","shell.execute_reply.started":"2025-03-31T02:04:41.586744Z","shell.execute_reply":"2025-03-31T02:04:41.606067Z"}},"outputs":[{"name":"stdout","text":"Image Size: <built-in method size of Tensor object at 0x7e586f9ede40>\nTarget: {'boxes': tensor([[ 79., 105., 109., 142.],\n        [185., 100., 226., 144.],\n        [325.,  90., 360., 141.]]), 'labels': tensor([2, 1, 2])}\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load full dataset\ndataset = CustomDataset(\"/kaggle/input/face-mask-detection\")\n\n# Generate train-test split indices\nindices = list(range(len(dataset)))\ntrain_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n\n# Create subsets\ntrain_dataset = torch.utils.data.Subset(dataset, train_indices)\ntest_dataset = torch.utils.data.Subset(dataset, test_indices)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:43.578118Z","iopub.execute_input":"2025-03-31T02:04:43.578427Z","iopub.status.idle":"2025-03-31T02:04:43.586572Z","shell.execute_reply.started":"2025-03-31T02:04:43.578404Z","shell.execute_reply":"2025-03-31T02:04:43.585871Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"import torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:45.249063Z","iopub.execute_input":"2025-03-31T02:04:45.249349Z","iopub.status.idle":"2025-03-31T02:04:45.253508Z","shell.execute_reply.started":"2025-03-31T02:04:45.249327Z","shell.execute_reply":"2025-03-31T02:04:45.252609Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:45.849906Z","iopub.execute_input":"2025-03-31T02:04:45.850238Z","iopub.status.idle":"2025-03-31T02:04:45.913715Z","shell.execute_reply.started":"2025-03-31T02:04:45.850208Z","shell.execute_reply":"2025-03-31T02:04:45.912780Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":95},{"cell_type":"code","source":"# Define optimizer\nlearning_rate = 0.005  # Try different values (e.g., 0.001, 0.01)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:45:54.198845Z","iopub.execute_input":"2025-03-31T01:45:54.199179Z","iopub.status.idle":"2025-03-31T01:45:54.204436Z","shell.execute_reply.started":"2025-03-31T01:45:54.199150Z","shell.execute_reply":"2025-03-31T01:45:54.203578Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:46:32.165572Z","iopub.execute_input":"2025-03-31T01:46:32.165939Z","iopub.status.idle":"2025-03-31T01:46:32.169513Z","shell.execute_reply.started":"2025-03-31T01:46:32.165910Z","shell.execute_reply":"2025-03-31T01:46:32.168748Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# Training loop\nnum_epochs = 5  # Try different values\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]  # Now img is a tensor\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:47:02.139317Z","iopub.execute_input":"2025-03-31T01:47:02.139678Z","iopub.status.idle":"2025-03-31T01:58:17.127897Z","shell.execute_reply.started":"2025-03-31T01:47:02.139647Z","shell.execute_reply":"2025-03-31T01:58:17.126926Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5], Loss: 73.5413\nEpoch [2/5], Loss: 49.2875\nEpoch [3/5], Loss: 40.4981\nEpoch [4/5], Loss: 34.5461\nEpoch [5/5], Loss: 30.7103\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:59:25.738250Z","iopub.execute_input":"2025-03-31T01:59:25.738582Z","iopub.status.idle":"2025-03-31T01:59:25.742299Z","shell.execute_reply.started":"2025-03-31T01:59:25.738557Z","shell.execute_reply":"2025-03-31T01:59:25.741311Z"}},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":"## if we try for 0.001 learning rate and for 7 epochs than result is look like","metadata":{}},{"cell_type":"code","source":"# Define optimizer\nlearning_rate = 0.001  # Try different values (e.g., 0.001, 0.01)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n\n# Training loop\nnum_epochs = 7  # Try different values\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]  # Now img is a tensor\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T02:04:53.813757Z","iopub.execute_input":"2025-03-31T02:04:53.814138Z","iopub.status.idle":"2025-03-31T02:20:38.344098Z","shell.execute_reply.started":"2025-03-31T02:04:53.814112Z","shell.execute_reply":"2025-03-31T02:20:38.343067Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/7], Loss: 92.1012\nEpoch [2/7], Loss: 57.4886\nEpoch [3/7], Loss: 48.9330\nEpoch [4/7], Loss: 44.3661\nEpoch [5/7], Loss: 40.3074\nEpoch [6/7], Loss: 37.4667\nEpoch [7/7], Loss: 34.4642\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}